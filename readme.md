# ğŸ‡ºğŸ‡¸ US Treasury Data Engineering Pipeline  
Medallion Architecture (Bronze â†’ Silver â†’ Gold) using Airflow, Spark & PostgreSQL

---

## ğŸ“Œ Project Overview

This project builds an end-to-end Data Engineering pipeline using official US Treasury Fiscal Data APIs.

It demonstrates:

- API-based data ingestion
- Medallion Architecture (Bronze / Silver / Gold layers)
- Distributed processing with Apache Spark
- Orchestration with Apache Airflow
- Data warehouse loading into PostgreSQL
- Fully containerized environment using Docker Compose

The goal is to simulate a production-grade modern data platform.

---

## ğŸ— Architecture

```
Raw API Data â†’ Bronze (Raw) â†’ Silver (Cleaned) â†’ Gold (Aggregated) â†’ PostgreSQL
     â†‘              â†‘              â†‘
   Airflow        Spark          Spark
```

### Components

- **Apache Airflow** â†’ Orchestrates pipeline tasks
- **Apache Spark** â†’ Performs distributed transformations
- **PostgreSQL** â†’ Stores analytics-ready data
- **Docker Compose** â†’ Container orchestration

---

## ğŸ“Š Data Sources

Data is extracted from:

- `operating_cash_balance`
- `public_debt_transactions`

Source:
https://api.fiscaldata.treasury.gov/

---

## ğŸ¥‰ Bronze Layer (Raw)

- Stores raw API responses
- Schema-on-read
- Immutable append-only storage
- Stored in Parquet format
- Adds ingestion metadata:
  - ingestion_timestamp
  - source_system

Purpose:
Preserve original data exactly as received.

---

## ğŸ¥ˆ Silver Layer (Cleaned)

- Standardizes data types
- Handles null values
- Deduplicates records
- Converts date fields
- Ensures numeric consistency

Purpose:
Create reliable, analytics-ready structured data.

---

## ğŸ¥‡ Gold Layer (Business Logic)

- Aggregated metrics
- Monthly debt trends
- Fiscal year summaries
- Transaction breakdowns

Purpose:
Deliver data ready for dashboards or BI tools.

---

## ğŸ“‚ Project Structure

```
FINANCE/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ treasury_dag.py           # Airflow DAG definition
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bronze/                   # Raw ingested data (Parquet)
â”‚   â”œâ”€â”€ silver/                   # Cleaned transformed data (Parquet)
â”‚   â””â”€â”€ gold/                     # Aggregated data (Parquet)
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py               # Data extraction from Treasury API
â”‚   â”œâ”€â”€ transform.py             # Data transformation logic
â”‚   â”œâ”€â”€ transform.ipynb          # Transformation notebooks
â”‚   â””â”€â”€ load.py                  # Database loading functions
â”œâ”€â”€ raw_data/                    # Raw CSV data from API
â”œâ”€â”€ spark_jobs/
â”‚   â”œâ”€â”€ bronze_ingest.py         # Spark job for bronze layer
â”‚   â”œâ”€â”€ silver_transform.py      # Spark job for silver layer
â”‚   â””â”€â”€ gold_aggregations.py     # Spark job for gold layer
â”œâ”€â”€ main.py                      # Main pipeline orchestration
â”œâ”€â”€ docker-compose.yml           # Docker services configuration
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ readme.md                    # Project documentation
```

---

## ğŸš€ Running the Project

### 1ï¸âƒ£ Start Services

```bash
docker compose up --build
```

Services:
- Airflow UI â†’ http://localhost:8080
- Spark UI â†’ http://localhost:8081
- PostgreSQL â†’ localhost:5432

---

### 2ï¸âƒ£ Airflow Login

```
Username: admin
Password: admin
```

Trigger the DAG:
`treasury_data_pipeline`

---

## ğŸ§  Technical Highlights

- Containerized distributed Spark environment
- Medallion data lake design pattern
- Separation of compute, orchestration, and storage
- Modular ETL structure
- Production-style folder organization
- Reproducible local development environment

---

## ğŸ’¡ Why This Project Matters

This project demonstrates:

- Understanding of modern data lake architecture
- Spark-based transformation pipelines
- Orchestrated workflows using Airflow
- Data warehouse loading patterns
- Real-world API ingestion handling

It reflects how real-world financial data platforms are structured.

---

## ğŸ”® Future Improvements

- Incremental loading logic
- Data quality validation checks
- Partitioned Parquet storage
- Star schema in Gold layer
- CI/CD integration
- Cloud deployment (AWS/GCP)

---

## ğŸ‘¨â€ğŸ’» Author

**Moavia Mahmood**  
Data Engineer
Focused on building scalable data systems and distributed processing pipelines.
